{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b8c827-fd08-4db7-8d8a-f7548a2fae53",
   "metadata": {},
   "source": [
    "- The Transformers library provides pretrained models and tools for NLP tasks.\n",
    "- Pytorch library: A deep learning framework for building and training neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbfdb07-9f12-4064-8b81-4cb5740cfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove virtual environment\n",
    "# pipenv --rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cbcf20-6ba1-44d7-8b57-1cb34ecd9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pipenv --python 3.10.0\n",
    "# !pipenv install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf75735-a3ed-49b5-acd7-93a1bc30282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pipenv shell\n",
    "# !pipenv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5839bd32-f9cf-4954-b638-d2ff5e1890c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m\u001b[1mInstalling \u001b[32m\u001b[1mtransformers\u001b[39m\u001b[22m...\u001b[39m\u001b[22m\n",
      "\u001b[K\u001b[39m\u001b[1mAdding\u001b[39m\u001b[22m \u001b[32m\u001b[1mtransformers\u001b[39m\u001b[22m \u001b[39m\u001b[1mto Pipfile's\u001b[39m\u001b[22m \u001b[33m\u001b[1m[packages]\u001b[39m\u001b[22m\u001b[39m\u001b[1m...\u001b[39m\u001b[22m\n",
      "\u001b[K\u001b[?25h‚úî Installation Succeeded\u001b[0m \n",
      "\u001b[39m\u001b[1mInstalling \u001b[32m\u001b[1mtorch\u001b[39m\u001b[22m...\u001b[39m\u001b[22m\n",
      "\u001b[K\u001b[39m\u001b[1mAdding\u001b[39m\u001b[22m \u001b[32m\u001b[1mtorch\u001b[39m\u001b[22m \u001b[39m\u001b[1mto Pipfile's\u001b[39m\u001b[22m \u001b[33m\u001b[1m[packages]\u001b[39m\u001b[22m\u001b[39m\u001b[1m...\u001b[39m\u001b[22m\n",
      "\u001b[K\u001b[?25h‚úî Installation Succeeded\u001b[0m \n",
      "\u001b[39m\u001b[1mPipfile.lock not found, creating...\u001b[39m\u001b[22m\n",
      "\u001b[39m\u001b[22mLocking\u001b[39m\u001b[22m \u001b[33m\u001b[22m[dev-packages]\u001b[39m\u001b[22m \u001b[39m\u001b[22mdependencies...\u001b[39m\u001b[22m\n",
      "\u001b[39m\u001b[22mLocking\u001b[39m\u001b[22m \u001b[33m\u001b[22m[packages]\u001b[39m\u001b[22m \u001b[39m\u001b[22mdependencies...\u001b[39m\u001b[22m\n",
      "\u001b[KBuilding requirements...\n",
      "\u001b[KResolving dependencies...\n",
      "\u001b[K\u001b[?25h\u001b[32m\u001b[22m‚úî Success!\u001b[39m\u001b[22m\u001b[0m \n",
      "\u001b[33m\u001b[22mWarning: /Users/cc62/.asdf/installs/python/3.9.0/lib/python3.9/site-packages/pipenv/resolver.py:11: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\u001b[39m\u001b[22m\n",
      "\u001b[39m\u001b[1mUpdated Pipfile.lock (4607c2)!\u001b[39m\u001b[22m\n",
      "\u001b[39m\u001b[1mInstalling dependencies from Pipfile.lock (4607c2)...\u001b[39m\u001b[22m\n",
      "  üêç   \u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m 2/2 ‚Äî \u001b[30m\u001b[22m00:00:00\u001b[39m\u001b[22m2 ‚Äî \u001b[30m\u001b[22m00:00:00\u001b[39m\u001b[22m  üêç   \u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m\u001b[30m\u001b[22m‚ñâ\u001b[39m\u001b[22m 1/2 ‚Äî \u001b[30m\u001b[22m00:00:00\u001b[39m\u001b[22m  üêç   \u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m 2/2 ‚Äî \u001b[30m\u001b[22m00:00:00\u001b[39m\u001b[22m\n",
      "To activate this project's virtualenv, run \u001b[33m\u001b[22mpipenv shell\u001b[39m\u001b[22m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33m\u001b[22mpipenv run\u001b[39m\u001b[22m.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pipenv install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf58873-5379-4e7a-b1d4-d2b4ad4b83cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching subshell in virtual environment...\n",
      " . /Users/cc62/.local/share/virtualenvs/tokenization-fETcUV0L/bin/activate\n",
      "Restored session: Tue May 20 14:52:37 EDT 2025\n",
      "\u001b[m\u001b[m\u001b[m\u001b[J\u001b[39mcc62@PU-C02FP1M7MD6R \u001b[39m/usr/local/Cellar/solr/9.8.1/server/solr/tokenization \u001b[39m[main]\u001b[39m\n",
      "% \u001b[K\u001b[?2004h . /Users/cc62/.local/share/virtualenvs/tokenization-fETcUV0L/bin/activate\u001b[?2004l\n",
      "\u001b[m\u001b[m\u001b[m\u001b[J(tokenization) \u001b[39mcc62@PU-C02FP1M7MD6R \u001b[39m/usr/local/Cellar/solr/9.8.1/server/solr/tokenization \u001b[39m[main]\u001b[39m\n",
      "% \u001b[K\u001b[?2004h"
     ]
    }
   ],
   "source": [
    "# Activate pipenv\n",
    "# !pipenv shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2bacd-4504-4b3b-937b-e803754d95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show versions: \n",
    "!pipenv run pip freeze | grep tensorflow\n",
    "!pipenv run pip freeze | grep torch\n",
    "\n",
    "# python version - executable path\n",
    "!python --version\n",
    "# # python version\n",
    "# import sys\n",
    "# print(sys.executable)\n",
    "# print(sys.version)\n",
    "# print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cbdaf-7d29-4056-91e2-b75e125b7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "bert_inputs = bert_tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(\"Token IDs:\", bert_inputs['input_ids'])\n",
    "\n",
    "attention_mask = bert_inputs['attention_mask']\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "token_type_ids = bert_inputs['token_type_ids']\n",
    "print(\"Token Type IDs:\", token_type_ids)\n",
    "\n",
    "# Print the tokens themselves to understand the splits\n",
    "tokens = bert_tokenizer.convert_ids_to_tokens(bert_inputs['input_ids'][0])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebadcc-8a08-44ec-bc9f-e783833e389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Obtain the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the last hidden state (embeddings)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Print the dimensions of the embeddings\n",
    "print(\"Shape of the last hidden state (embeddings):\", last_hidden_states.shape)\n",
    "\n",
    "# Print embeddings for each token along with their vector dimension\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "for token, embedding in zip(tokens, last_hidden_states[0]):\n",
    "    print(f\"Token: {token}, Embedding Dimension: {embedding.shape}, Embedding (first 5 components): {embedding[:5]}...\")  # Display first 5 components for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e05901-7aca-475d-ac8d-38efdc0e1cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
