{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b8c827-fd08-4db7-8d8a-f7548a2fae53",
   "metadata": {},
   "source": [
    "- The Transformers library provides pretrained models and tools for NLP tasks.\n",
    "- Pytorch library: A deep learning framework for building and training neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5839bd32-f9cf-4954-b638-d2ff5e1890c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mInstalling transformers...\u001b[0m\n",
      "✔ Installation Succeeded\n",
      "\u001b[1;32mInstalling torch...\u001b[0m\n",
      "✔ Installation Succeeded\n",
      "\u001b[1;32mInstalling numpy==2.2.6...\u001b[0m\n",
      "✔ Installation Succeeded\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(0ab491)...\u001b[0m\n",
      "\u001b[32mAll dependencies are now up-to-date!\u001b[0m\n",
      "\u001b[1;32mUpgrading\u001b[0m transformers, torch, \u001b[33mnumpy\u001b[0m==\u001b[1;36m2.2\u001b[0m.\u001b[1;36m6\u001b[0m in \u001b[39m dependencies.\u001b[0m\n",
      "\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies....\n",
      "\u001b[2K✔ Success! Locking packages...\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Locking packages...\n",
      "\u001b[1A\u001b[2K\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies....\n",
      "\u001b[2K✔ Success! Locking packages...\n",
      "\u001b[2K\u001b[32m⠇\u001b[0m Locking packages...\n",
      "\u001b[1A\u001b[2K\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies....\n",
      "\u001b[2K✔ Success! Locking packages...\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m Locking packages...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(0ab491)...\u001b[0m\n",
      "\u001b[32mAll dependencies are now up-to-date!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(0ab491)...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pipenv uninstall torch numpy transformers\n",
    "!pipenv install transformers torch numpy==2.2.6\n",
    "# !pipenv run pip install\n",
    "\n",
    "# import os \n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7a2bacd-4504-4b3b-937b-e803754d95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==2.2.2\n",
      "numpy==2.2.6\n",
      "/Users/cc62/.local/share/virtualenvs/solr-si4nbHVf/bin/python\n",
      "3.11.0 (main, May 20 2025, 15:32:54) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "sys.version_info(major=3, minor=11, micro=0, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "# Show versions: \n",
    "!pipenv run pip freeze | grep tensorflow\n",
    "!pipenv run pip freeze | grep torch\n",
    "!pipenv run pip freeze | grep numpy\n",
    "# Show Python version\n",
    "import os\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "667cbdaf-7d29-4056-91e2-b75e125b7baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
      "          1012,   102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Token Type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Tokens: ['[CLS]', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# Load BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "bert_inputs = bert_tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(\"Token IDs:\", bert_inputs['input_ids'])\n",
    "\n",
    "attention_mask = bert_inputs['attention_mask']\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "token_type_ids = bert_inputs['token_type_ids']\n",
    "print(\"Token Type IDs:\", token_type_ids)\n",
    "\n",
    "# Print the tokens themselves to understand the splits\n",
    "tokens = bert_tokenizer.convert_ids_to_tokens(bert_inputs['input_ids'][0])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3aebadcc-8a08-44ec-bc9f-e783833e389a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'get_default_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load pre-trained model and tokenizer\u001b[39;00m\n\u001b[32m      5\u001b[39m tokenizer = BertTokenizer.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mBertModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbert-base-uncased\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Define the text\u001b[39;00m\n\u001b[32m      9\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mThe quick brown fox jumps over the lazy dog.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/solr-si4nbHVf/lib/python3.11/site-packages/transformers/modeling_utils.py:308\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    310\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/solr-si4nbHVf/lib/python3.11/site-packages/transformers/modeling_utils.py:4292\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4290\u001b[39m \u001b[38;5;66;03m# Potentially detect context manager or global device, and use it (only if no device_map was provided)\u001b[39;00m\n\u001b[32m   4291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[32m-> \u001b[39m\u001b[32m4292\u001b[39m     device_in_context = \u001b[43mget_torch_context_manager_or_global_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device_in_context == torch.device(\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   4294\u001b[39m         \u001b[38;5;66;03m# TODO Cyril: raise an error instead of the warning in v4.53 (and change the test to check for raise instead of success)\u001b[39;00m\n\u001b[32m   4295\u001b[39m         logger.warning(\n\u001b[32m   4296\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mWe detected that you are using `from_pretrained` with a meta device context manager or `torch.set_default_device(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)`\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   4297\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThis is an anti-pattern and will raise an Error in version v4.53\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf you want to initialize a model on the meta device, use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4298\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthe context manager or global device with `from_config`, or `ModelClass(config)`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4299\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/solr-si4nbHVf/lib/python3.11/site-packages/transformers/modeling_utils.py:321\u001b[39m, in \u001b[36mget_torch_context_manager_or_global_device\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03mTest if a device context manager is currently in use, or if it is not the case, check if the default device\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03mis not \"cpu\". This is used to infer the correct device to load the model on, in case `device_map` is not provided.\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m device_in_context = torch.tensor([]).device\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m default_device = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_default_device\u001b[49m()\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# This case means no context manager was used -> we still check if the default that was potentially set is not cpu\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_in_context == default_device:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/solr-si4nbHVf/lib/python3.11/site-packages/torch/__init__.py:1938\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'get_default_device'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Obtain the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the last hidden state (embeddings)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Print the dimensions of the embeddings\n",
    "print(\"Shape of the last hidden state (embeddings):\", last_hidden_states.shape)\n",
    "\n",
    "# Print embeddings for each token along with their vector dimension\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "for token, embedding in zip(tokens, last_hidden_states[0]):\n",
    "    print(f\"Token: {token}, Embedding Dimension: {embedding.shape}, Embedding (first 5 components): {embedding[:5]}...\")  # Display first 5 components for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36770cce-159f-4c12-83c2-1678860bb1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
